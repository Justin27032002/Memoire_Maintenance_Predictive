{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7d56cff-bbbc-452e-921d-95cf74532dd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Notebook d'harmonisation et de nettoyage des colonnes pour la préparation des données ML.\n",
    "\n",
    "Ce notebook assure l'uniformité des datasets générés en harmonisant les colonnes entre tous\n",
    "les fichiers train/test. Il supprime les colonnes inutiles, vérifie la cohérence et prépare\n",
    "les données pour la modélisation en garantissant que tous les fichiers ont exactement les\n",
    "mêmes features. Il effectue les opérations suivantes :\n",
    "\n",
    "1. **Analyse des colonnes disponibles** :\n",
    "   - Parcours du dossier datasets_par_pourcentage/\n",
    "   - Inventaire des colonnes présentes dans chaque fichier CSV\n",
    "   - Échantillonnage sur 5 premiers fichiers pour analyse rapide\n",
    "   - Création d'un set all_columns_before avec toutes les colonnes uniques\n",
    "   - Comptage et affichage du nombre total de colonnes trouvées\n",
    "   - Identification des variations entre fichiers\n",
    "\n",
    "2. **Définition des colonnes communes à conserver** :\n",
    "   - Liste COLONNES_COMMUNES de 41 colonnes essentielles :\n",
    "     * Métadonnées : Defaut, fichier, unit_id, pourcentage, pourcentage_reel\n",
    "     * Variables temporelles : Nref, N\n",
    "     * Températures (11) : T_amb, T_c_in/out, T_rec_*, T_cc_*, T_t_*\n",
    "     * Pressions (11) : p_amb, p_c_in/out, p_rec_*, p_cc_*, p_t_*\n",
    "     * Débits (12) : m_amb, m_c_in/out, m_rec_*, m_cc_*, m_t_*, m_fuel\n",
    "   - Liste COLONNES_A_SUPPRIMER : Power_*, Eff_*, Beta_*, Is_Eff_*, etc.\n",
    "   - Vérification de présence réelle dans les données\n",
    "   - Rapport colonnes présentes vs absentes\n",
    "\n",
    "3. **Nettoyage et harmonisation des fichiers** :\n",
    "   - Boucle sur tous les fichiers CSV du dossier\n",
    "   - Conservation uniquement des colonnes dans COLONNES_COMMUNES ET présentes\n",
    "   - Suppression automatique des colonnes non désirées\n",
    "   - Tracking des colonnes supprimées par fichier dans stats_nettoyage\n",
    "   - Sauvegarde in-place des fichiers nettoyés\n",
    "   - Gestion des erreurs avec try/except et logging\n",
    "   - Progression affichée tous les 10 fichiers\n",
    "\n",
    "4. **Vérification finale de cohérence** :\n",
    "   - Rechargement d'un échantillon post-nettoyage\n",
    "   - Vérification que tous les fichiers ont les mêmes colonnes\n",
    "   - Calcul de la réduction : colonnes avant → après\n",
    "   - Liste alphabétique des colonnes finales conservées\n",
    "   - Détection des fichiers incohérents (normalement 0)\n",
    "   - Validation \"PARFAIT\" si uniformité complète\n",
    "\n",
    "5. **Analyse des features pour la modélisation** :\n",
    "   - Catégorisation automatique par préfixe :\n",
    "     * Températures : T_* (11 capteurs)\n",
    "     * Pressions : p_* (11 capteurs)\n",
    "     * Débits : m_* (12 capteurs)\n",
    "     * Autres : Nref, N\n",
    "   - Exclusion des métadonnées de l'analyse\n",
    "   - Comptage par catégorie pour validation\n",
    "   - Préparation pour la sélection de features\n",
    "\n",
    "6. **Statistiques sur les données nettoyées** :\n",
    "   - Analyse sur dataset_test_p50.csv comme échantillon représentatif\n",
    "   - Comptage des features numériques utilisables\n",
    "   - Calcul du taux de défaut dans l'échantillon\n",
    "   - Vérification des valeurs manquantes (NaN)\n",
    "   - Statistiques par colonne si valeurs manquantes détectées\n",
    "   - Confirmation \"Aucune valeur manquante\" si données complètes\n",
    "\n",
    "7. **Génération du rapport de nettoyage** :\n",
    "   - Création rapport JSON avec métadonnées complètes\n",
    "   - Statistiques globales : fichiers traités, colonnes avant/après\n",
    "   - Liste des colonnes finales conservées\n",
    "   - Décompte des features par catégorie\n",
    "   - Exemples de colonnes supprimées (5 premiers)\n",
    "   - Sauvegarde : rapport_nettoyage_colonnes.json\n",
    "   - Timestamp d'exécution pour traçabilité\n",
    "\n",
    "Paramètres et configuration :\n",
    "- DOSSIER : /Volumes/dbe_dbx_internships/justin/predictive_maintenance/datasets_par_pourcentage\n",
    "- Colonnes théoriques : 41 colonnes définies\n",
    "- Colonnes à supprimer : 10 colonnes identifiées comme non pertinentes\n",
    "- Format de sortie : CSV harmonisé, même structure pour tous\n",
    "\n",
    "Résultats produits :\n",
    "- Tous les fichiers CSV avec exactement les mêmes colonnes\n",
    "- 0 valeur manquante (données complètes)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42c9948c-04d9-4e4c-8c03-431a29227aa7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\nMAINTENANCE PRÉDICTIVE - NETTOYAGE DES DONNÉES\nNotebook 3bis: Harmonisation des Colonnes entre Datasets\n================================================================================\n\nDate d'exécution: 2025-06-18 08:55:26\n\n\n PHASE 1: ANALYSE DES COLONNES DISPONIBLES\n------------------------------------------------------------\n Analyse des colonnes présentes dans chaque fichier...\n\n dataset_test_p0.csv:\n   Nombre de colonnes: 105\n\n dataset_test_p1.csv:\n   Nombre de colonnes: 105\n\n dataset_test_p10.csv:\n   Nombre de colonnes: 105\n\n dataset_test_p100.csv:\n   Nombre de colonnes: 105\n\n dataset_test_p2.csv:\n   Nombre de colonnes: 105\n\n✓ Total colonnes uniques trouvées: 105\n\n\n PHASE 2: DÉFINITION DES COLONNES COMMUNES\n------------------------------------------------------------\n✓ Colonnes à conserver: 41\n✓ Colonnes à supprimer: 11\n\n Statut des colonnes:\n   - Présentes: 39\n   - Absentes: 2\n\n  Colonnes absentes qui seront ignorées:\n   - unit_id\n   - pourcentage_reel\n\n\n PHASE 3: NETTOYAGE ET HARMONISATION\n------------------------------------------------------------\n Nettoyage en cours...\n    10/22 fichiers traités...\n    20/22 fichiers traités...\n\n Nettoyage terminé!\n   - Fichiers traités: 22\n   - Fichiers avec erreurs: 0\n\n\n PHASE 4: VÉRIFICATION FINALE\n------------------------------------------------------------\n Vérification de la cohérence...\n\n Colonnes finales uniques: 39\n Réduction: 105 → 39 colonnes\n\n Liste des colonnes finales (ordre alphabétique):\n    1. Defaut\n    2. N\n    3. Nref\n    4. T_amb\n    5. T_c_in\n    6. T_c_out\n    7. T_cc_in\n    8. T_cc_out\n    9. T_rec_c_in\n   10. T_rec_c_out\n   11. T_rec_h_in\n   12. T_rec_h_out\n   13. T_t_in\n   14. T_t_out\n   15. fichier\n   16. m_amb\n   17. m_c_in\n   18. m_c_out\n   19. m_cc_in\n   20. m_cc_out\n   21. m_fuel\n   22. m_rec_c_in\n   23. m_rec_c_out\n   24. m_rec_h_in\n   25. m_rec_h_out\n   26. m_t_in\n   27. m_t_out\n   28. p_amb\n   29. p_c_in\n   30. p_c_out\n   31. p_cc_in\n   32. p_cc_out\n   33. p_rec_c_in\n   34. p_rec_c_out\n   35. p_rec_h_in\n   36. p_rec_h_out\n   37. p_t_in\n   38. p_t_out\n   39. pourcentage\n\n  1 fichiers avec colonnes différentes détectés\n\n\n PHASE 5: ANALYSE DES FEATURES POUR LA MODÉLISATION\n------------------------------------------------------------\n Répartition des features par catégorie:\n\nTempératures: 11 features\n\nPressions: 11 features\n\nDébits: 12 features\n\nAutres: 2 features\n   - N\n   - Nref\n\n\n PHASE 6: STATISTIQUES SUR LES DONNÉES NETTOYÉES\n------------------------------------------------------------\n Analyse sur dataset p50%:\n   - Échantillons: 1109\n   - Features numériques: 36\n   - Taux de défaut: 41.48%\n\n Aucune valeur manquante!\n\n\n GÉNÉRATION DU RAPPORT DE NETTOYAGE\n------------------------------------------------------------\n Rapport sauvegardé: rapport_nettoyage_colonnes.json\n\n\n================================================================================\nRÉSUMÉ DU NETTOYAGE\n================================================================================\n\n NETTOYAGE COMPLÉTÉ AVEC SUCCÈS:\n   - 22 fichiers harmonisés\n   - 66 colonnes supprimées\n   - 39 colonnes conservées\n\n FEATURES DISPONIBLES POUR LA MODÉLISATION:\n   - 11 capteurs de température\n   - 11 capteurs de pression\n   - 12 capteurs de débit\n   - 2 autres features\n\n PROCHAINES ÉTAPES:\n   1. Normalisation des features (Notebook 4)\n   2. Sélection des features pertinentes (Notebook 5)\n   3. Modélisation et optimisation (Notebooks 6-8)\n\n REMARQUE IMPORTANTE:\n   Les colonnes théoriques suivantes n'étaient pas présentes dans les données:\n   - Power_Shaft, Eff_Cycle, Power_Comp, Power_Turb\n   - Beta_Comp, Beta_Turb, Is_Eff_Comp, Is_Eff_Turb\n   → Ces features ne seront pas utilisées dans la modélisation\n\n================================================================================\nFIN DU NOTEBOOK 3bis - DONNÉES NETTOYÉES ET HARMONISÉES\n================================================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MAINTENANCE PRÉDICTIVE - NETTOYAGE DES DONNÉES\")\n",
    "print(\"Notebook 3bis: Harmonisation des Colonnes entre Datasets\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nDate d'exécution: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "DOSSIER = \"/Volumes/dbe_dbx_internships/justin/predictive_maintenance/datasets_par_pourcentage\"\n",
    "\n",
    "print(\" PHASE 1: ANALYSE DES COLONNES DISPONIBLES\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# === Analyse initiale des colonnes ===\n",
    "print(\" Analyse des colonnes présentes dans chaque fichier...\")\n",
    "\n",
    "fichiers = [f for f in os.listdir(DOSSIER) if f.endswith(\".csv\") and not f.startswith(\"dataset_metadata\")]\n",
    "colonnes_par_fichier = {}\n",
    "all_columns_before = set()\n",
    "\n",
    "for fichier in fichiers[:5]:  # Échantillon pour analyse\n",
    "    df = pd.read_csv(os.path.join(DOSSIER, fichier))\n",
    "    colonnes_par_fichier[fichier] = list(df.columns)\n",
    "    all_columns_before.update(df.columns)\n",
    "    print(f\"\\n {fichier}:\")\n",
    "    print(f\"   Nombre de colonnes: {len(df.columns)}\")\n",
    "\n",
    "print(f\"\\n✓ Total colonnes uniques trouvées: {len(all_columns_before)}\")\n",
    "\n",
    "# === Définition des colonnes à garder ===\n",
    "print(f\"\\n\\n PHASE 2: DÉFINITION DES COLONNES COMMUNES\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Colonnes communes RÉELLES (sans celles qui n'existent pas)\n",
    "COLONNES_COMMUNES = [\n",
    "    # Identifiants et métadonnées\n",
    "    \"Defaut\", \"fichier\", \"unit_id\", \"pourcentage\", \"pourcentage_reel\",\n",
    "    \n",
    "    # Variables temporelles\n",
    "     \"Nref\", \"N\",\n",
    "    \n",
    "    # Températures\n",
    "    \"T_amb\", \"T_c_in\", \"T_c_out\", \"T_rec_c_in\", \"T_rec_c_out\", \n",
    "    \"T_cc_in\", \"T_cc_out\", \"T_t_in\", \"T_t_out\", \"T_rec_h_in\", \"T_rec_h_out\",\n",
    "    \n",
    "    # Pressions\n",
    "    \"p_amb\", \"p_c_in\", \"p_c_out\", \"p_rec_c_in\", \"p_rec_c_out\",\n",
    "    \"p_cc_in\", \"p_cc_out\", \"p_t_in\", \"p_t_out\", \"p_rec_h_in\", \"p_rec_h_out\",\n",
    "    \n",
    "    # Débits massiques\n",
    "    \"m_amb\", \"m_c_in\", \"m_c_out\", \"m_rec_c_in\", \"m_rec_c_out\",\n",
    "    \"m_cc_in\", \"m_cc_out\", \"m_t_in\", \"m_t_out\", \"m_rec_h_in\", \"m_rec_h_out\", \"m_fuel\"\n",
    "]\n",
    "\n",
    "# Colonnes à supprimer (qui peuvent exister dans certains fichiers)\n",
    "COLONNES_A_SUPPRIMER = [\n",
    "    \"Power_Comp\", \"Is_Eff_Turb\", \"Is_Eff_Comp\", \"Power_Shaft\", \"Eff_Cycle\",\n",
    "    \"Power_Turb\", \"Beta_Comp\", \"Beta_Turb\", \"Pdrop\", \"act_ash\", \"%\"\n",
    "]\n",
    "\n",
    "print(f\"✓ Colonnes à conserver: {len(COLONNES_COMMUNES)}\")\n",
    "print(f\"✓ Colonnes à supprimer: {len(COLONNES_A_SUPPRIMER)}\")\n",
    "\n",
    "# Vérifier quelles colonnes définies sont réellement présentes\n",
    "colonnes_presentes = []\n",
    "colonnes_absentes = []\n",
    "\n",
    "# Charger un fichier représentatif\n",
    "df_sample = pd.read_csv(os.path.join(DOSSIER, fichiers[0]))\n",
    "for col in COLONNES_COMMUNES:\n",
    "    if col in df_sample.columns:\n",
    "        colonnes_presentes.append(col)\n",
    "    else:\n",
    "        colonnes_absentes.append(col)\n",
    "\n",
    "print(f\"\\n Statut des colonnes:\")\n",
    "print(f\"   - Présentes: {len(colonnes_presentes)}\")\n",
    "print(f\"   - Absentes: {len(colonnes_absentes)}\")\n",
    "\n",
    "if colonnes_absentes:\n",
    "    print(f\"\\n  Colonnes absentes qui seront ignorées:\")\n",
    "    for col in colonnes_absentes:\n",
    "        print(f\"   - {col}\")\n",
    "\n",
    "# === Nettoyage des fichiers ===\n",
    "print(f\"\\n\\n PHASE 3: NETTOYAGE ET HARMONISATION\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Statistiques de nettoyage\n",
    "stats_nettoyage = {\n",
    "    'fichiers_traites': 0,\n",
    "    'colonnes_supprimees': {},\n",
    "    'erreurs': []\n",
    "}\n",
    "\n",
    "print(\" Nettoyage en cours...\")\n",
    "\n",
    "for i, nom_fichier in enumerate(fichiers):\n",
    "    try:\n",
    "        chemin_fichier = os.path.join(DOSSIER, nom_fichier)\n",
    "        df = pd.read_csv(chemin_fichier)\n",
    "        \n",
    "        colonnes_initiales = set(df.columns)\n",
    "        \n",
    "        # Ne garder que les colonnes qui existent ET sont dans COLONNES_COMMUNES\n",
    "        cols_to_keep = [col for col in df.columns if col in colonnes_presentes]\n",
    "        \n",
    "        # Identifier les colonnes supprimées\n",
    "        colonnes_supprimees = colonnes_initiales - set(cols_to_keep)\n",
    "        if colonnes_supprimees:\n",
    "            stats_nettoyage['colonnes_supprimees'][nom_fichier] = list(colonnes_supprimees)\n",
    "        \n",
    "        # Appliquer le nettoyage\n",
    "        df_clean = df[cols_to_keep]\n",
    "        \n",
    "        # Sauvegarder\n",
    "        df_clean.to_csv(chemin_fichier, index=False)\n",
    "        stats_nettoyage['fichiers_traites'] += 1\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"    {i + 1}/{len(fichiers)} fichiers traités...\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        stats_nettoyage['erreurs'].append({\n",
    "            'fichier': nom_fichier,\n",
    "            'erreur': str(e)\n",
    "        })\n",
    "        print(f\"    Erreur sur {nom_fichier}: {e}\")\n",
    "\n",
    "print(f\"\\n Nettoyage terminé!\")\n",
    "print(f\"   - Fichiers traités: {stats_nettoyage['fichiers_traites']}\")\n",
    "print(f\"   - Fichiers avec erreurs: {len(stats_nettoyage['erreurs'])}\")\n",
    "\n",
    "# === Vérification finale ===\n",
    "print(f\"\\n\\n PHASE 4: VÉRIFICATION FINALE\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Vérifier la cohérence après nettoyage\n",
    "all_columns_after = set()\n",
    "sample_files = min(5, len(fichiers))\n",
    "\n",
    "print(\" Vérification de la cohérence...\")\n",
    "for fichier in fichiers[:sample_files]:\n",
    "    df = pd.read_csv(os.path.join(DOSSIER, fichier))\n",
    "    all_columns_after.update(df.columns)\n",
    "\n",
    "print(f\"\\n Colonnes finales uniques: {len(all_columns_after)}\")\n",
    "print(f\" Réduction: {len(all_columns_before)} → {len(all_columns_after)} colonnes\")\n",
    "\n",
    "# Afficher les colonnes finales\n",
    "print(f\"\\n Liste des colonnes finales (ordre alphabétique):\")\n",
    "colonnes_finales = sorted(all_columns_after)\n",
    "for i, col in enumerate(colonnes_finales, 1):\n",
    "    print(f\"   {i:2d}. {col}\")\n",
    "\n",
    "# Vérifier la cohérence parfaite\n",
    "fichiers_incoherents = []\n",
    "for fichier in fichiers:\n",
    "    df = pd.read_csv(os.path.join(DOSSIER, fichier))\n",
    "    if set(df.columns) != all_columns_after:\n",
    "        fichiers_incoherents.append(fichier)\n",
    "\n",
    "if fichiers_incoherents:\n",
    "    print(f\"\\n  {len(fichiers_incoherents)} fichiers avec colonnes différentes détectés\")\n",
    "else:\n",
    "    print(f\"\\n PARFAIT: Tous les fichiers ont exactement les mêmes colonnes!\")\n",
    "\n",
    "# === Analyse des features pour la modélisation ===\n",
    "print(f\"\\n\\n PHASE 5: ANALYSE DES FEATURES POUR LA MODÉLISATION\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Identifier les types de features\n",
    "features_categories = {\n",
    "    'Températures': [col for col in colonnes_finales if col.startswith('T_')],\n",
    "    'Pressions': [col for col in colonnes_finales if col.startswith('p_')],\n",
    "    'Débits': [col for col in colonnes_finales if col.startswith('m_')],\n",
    "    'Autres': [col for col in colonnes_finales if not any(col.startswith(prefix) for prefix in ['T_', 'p_', 'm_']) \n",
    "               and col not in ['Defaut', 'fichier', 'unit_id', 'pourcentage', 'pourcentage_reel']]\n",
    "}\n",
    "\n",
    "print(\" Répartition des features par catégorie:\")\n",
    "for categorie, features in features_categories.items():\n",
    "    print(f\"\\n{categorie}: {len(features)} features\")\n",
    "    if len(features) <= 10:  # Afficher si pas trop nombreuses\n",
    "        for feat in features:\n",
    "            print(f\"   - {feat}\")\n",
    "\n",
    "# === Statistiques sur un échantillon ===\n",
    "print(f\"\\n\\n PHASE 6: STATISTIQUES SUR LES DONNÉES NETTOYÉES\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Charger un dataset représentatif (p50)\n",
    "df_p50 = pd.read_csv(os.path.join(DOSSIER, \"dataset_test_p50.csv\"))\n",
    "\n",
    "# Features numériques seulement\n",
    "features_numeriques = [col for col in df_p50.columns \n",
    "                      if col not in ['Defaut', 'fichier', 'unit_id', 'pourcentage', 'pourcentage_reel']]\n",
    "\n",
    "print(f\" Analyse sur dataset p50%:\")\n",
    "print(f\"   - Échantillons: {len(df_p50)}\")\n",
    "print(f\"   - Features numériques: {len(features_numeriques)}\")\n",
    "print(f\"   - Taux de défaut: {df_p50['Defaut'].mean():.2%}\")\n",
    "\n",
    "# Vérifier les valeurs manquantes\n",
    "missing_stats = df_p50[features_numeriques].isnull().sum()\n",
    "if missing_stats.sum() > 0:\n",
    "    print(f\"\\n  Valeurs manquantes détectées:\")\n",
    "    for col, count in missing_stats[missing_stats > 0].items():\n",
    "        print(f\"   - {col}: {count} ({count/len(df_p50)*100:.1f}%)\")\n",
    "else:\n",
    "    print(f\"\\n Aucune valeur manquante!\")\n",
    "\n",
    "# === Rapport de nettoyage ===\n",
    "print(f\"\\n\\n GÉNÉRATION DU RAPPORT DE NETTOYAGE\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "rapport_nettoyage = {\n",
    "    'date_execution': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'statistiques': {\n",
    "        'fichiers_traites': stats_nettoyage['fichiers_traites'],\n",
    "        'colonnes_avant': len(all_columns_before),\n",
    "        'colonnes_apres': len(all_columns_after),\n",
    "        'erreurs': len(stats_nettoyage['erreurs'])\n",
    "    },\n",
    "    'colonnes_finales': colonnes_finales,\n",
    "    'features_par_categorie': {\n",
    "        cat: len(feats) for cat, feats in features_categories.items()\n",
    "    },\n",
    "    'colonnes_supprimees_exemples': list(stats_nettoyage['colonnes_supprimees'].items())[:5]\n",
    "}\n",
    "\n",
    "# Sauvegarder le rapport\n",
    "with open(os.path.join(DOSSIER, 'rapport_nettoyage_colonnes.json'), 'w') as f:\n",
    "    json.dump(rapport_nettoyage, f, indent=4)\n",
    "\n",
    "print(\" Rapport sauvegardé: rapport_nettoyage_colonnes.json\")\n",
    "\n",
    "# === Résumé final ===\n",
    "print(f\"\\n\\n\" + \"=\" * 80)\n",
    "print(\"RÉSUMÉ DU NETTOYAGE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n NETTOYAGE COMPLÉTÉ AVEC SUCCÈS:\")\n",
    "print(f\"   - {stats_nettoyage['fichiers_traites']} fichiers harmonisés\")\n",
    "print(f\"   - {len(all_columns_before) - len(all_columns_after)} colonnes supprimées\")\n",
    "print(f\"   - {len(colonnes_finales)} colonnes conservées\")\n",
    "\n",
    "print(f\"\\n FEATURES DISPONIBLES POUR LA MODÉLISATION:\")\n",
    "print(f\"   - {len(features_categories['Températures'])} capteurs de température\")\n",
    "print(f\"   - {len(features_categories['Pressions'])} capteurs de pression\") \n",
    "print(f\"   - {len(features_categories['Débits'])} capteurs de débit\")\n",
    "print(f\"   - {len(features_categories['Autres'])} autres features\")\n",
    "\n",
    "print(f\"\\n PROCHAINES ÉTAPES:\")\n",
    "print(f\"   1. Normalisation des features (Notebook 4)\")\n",
    "print(f\"   2. Sélection des features pertinentes (Notebook 5)\")\n",
    "print(f\"   3. Modélisation et optimisation (Notebooks 6-8)\")\n",
    "\n",
    "print(f\"\\n REMARQUE IMPORTANTE:\")\n",
    "print(f\"   Les colonnes théoriques suivantes n'étaient pas présentes dans les données:\")\n",
    "print(f\"   - Power_Shaft, Eff_Cycle, Power_Comp, Power_Turb\")\n",
    "print(f\"   - Beta_Comp, Beta_Turb, Is_Eff_Comp, Is_Eff_Turb\")\n",
    "print(f\"   → Ces features ne seront pas utilisées dans la modélisation\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FIN DU NOTEBOOK 3bis - DONNÉES NETTOYÉES ET HARMONISÉES\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Nettoyage_ Colonnes_Inexistantes_3bis",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
